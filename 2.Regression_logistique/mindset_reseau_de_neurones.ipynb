{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mindset d'un réseau de neurones avec la régression logistique\n",
    "\n",
    "Dans ce workshop, vous aller coder un réseau de neurones pour reconnaître des chats à partir d'une image. Vous apprendrez le mindset (= l'état d'esprit) du fonctionnement d'une réseau de neurones, et acquérir d'une manière générale une idée du deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- N'utilisez pas de boucles for ou while, à moins qu'on ne vous le demande explicitement.\n",
    "\n",
    "\n",
    "**Vous allez apprendre à:**\n",
    "- Construire l'architecture générale d'une modèle d'apprentissage, incluant:\n",
    "    - l'initialisation des paramètres\n",
    "    - le calcul de la fonction de coût et de son gradient\n",
    "    - l'utilisation d'un algorithme d'optimisation\n",
    "- Regrouper les trois fonctions ci-dessus pour le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Importons d'abord les packages suivant dont vous aurez besoin:\n",
    "- [numpy](www.numpy.org) est le package fondamental pour le calcul scientifique avec Python.\n",
    "- [h5py](http://www.h5py.org) est un package permettant d'interagir avec un dataset stocké dans un fichier H5.\n",
    "- [matplotlib](http://matplotlib.org) est une librairie connue pour afficher des graphiques en Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) et [scipy](https://www.scipy.org/) sont utilisés ici pour tester votre modèle avec vos propres photos à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Overview de la problématique##\n",
    "\n",
    "**Problématique**: \n",
    "On vous as donné un jeu de données (\"data.h5\") contenant:\n",
    "    - un training set de m_train images labellisées chacune comme étant un chat (y=1) ou non-chat(y=0)\n",
    "    - un test set de m_test images labellisées chacune comme étant un chat ou non-chat\n",
    "    - chaque image est de shape (num_px, num_px, 3) où 3 correspond aux trois channels RGB. Chaque image est donc carrée de longueur num_px et de largeur num_px.\n",
    "\n",
    "Vous allez construire un algorithme simple de reconnaissance d'images qui pourra classifier correctement les images de chats des images de non-chat.\n",
    "\n",
    "Explorons d'abord notre dataset. On va commencer par l'importer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "x_train_orig, y_train, x_test_orig, y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ajouté \"_orig\" à la fin des datasets d'images (train et test) car on va les traiter d'abord. En général, lorqu'on vous donne un jeu de données, il n'est jamais parfait. Vous devrez toujours passer par une étape de nettoyage qu'on appelle le préprocessing. Après cette étape, vous vous retrouverez avec x_train et x_test (y_train et y_test n'ont pas besoin de préprocessing)\n",
    "\n",
    "Chaque ligne de votre x_train_orig et y_test_orig est un array représentant une image. Vous pouvez visualiser un example en exécutant le code suivant. Vous pouvez aussi changer la valeur de `index` si vous voulez voir d'autres images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'une image\n",
    "index = 30\n",
    "plt.imshow(x_train_orig[index])\n",
    "if classes[np.squeeze(y_train[:, index])] == 'cat':\n",
    "    print(\"y = \" + str(y_train[:, index]) + \", c'est une photo de chat\")\n",
    "else:\n",
    "    print(\"y = \" + str(y_train[:, index]) + \", ce n'est pas une photo de chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de bugs en deep learning proviennent du fait que les dimensions de matrices/vecteurs ne sont pas correctes. Si avez les bonnes valeurs, vous éviterez de nombreux bugs.\n",
    "\n",
    "\n",
    "**Exercise:** Trouvez les bonnes valeurs de:\n",
    "    - m_train (nombre d'exemples de train)\n",
    "    - m_test (nombre d'exemples de test)\n",
    "    - num_px (=longueur =largeur d'une image)\n",
    "    \n",
    "Rappelez vous que x_train_orig est un numpy array de shape (m_train, num_px, num_px, 3). Par exemple, vous pouvez accéder à `m_train` en écrivant `x_train_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Début du code ### (≈ 3 lignes de code)\n",
    "m_train = None\n",
    "m_test = None\n",
    "num_px = None\n",
    "### Fin du code ###\n",
    "\n",
    "print (\"Nombre d'exemples de train: m_train = \" + str(m_train))\n",
    "print (\"Nombre d'exemples de test: m_test = \" + str(m_test))\n",
    "print (\"Longueur/Largeur de chaque image: num_px = \" + str(num_px))\n",
    "print (\"Chaque image est de size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"x_train shape: \" + str(x_train_orig.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"x_test shape: \" + str(x_test_orig.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attentu pour m_train, m_test and num_px**: \n",
    "\n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>m_train</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des raisons pratiques, vous devriez reshape vos images de shapes (num_px, num_px, 3) en un numpy array de shape (num_px $*$ num_px $*$ 3, 1). Après cela, nos jeux de données de train (et de test) seront des numpy array dans laquelle chaque colonne représente une image applatie (\"flatten\" en anglais). Il devrait y avoir m_train (respectivement m_test) colonnes.\n",
    "\n",
    "\n",
    "**Exercise:** \n",
    "Reshapez les jeux de données de train et de test de façon à ce que les images de size (num_px, num_px, 3) soient flatten en simples vecteurs de shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "Petite astuce quand vous voulez flatten une matrice X de shape (a,b,c,d) en une matrice X_flatten de shape (b$*$c$*$d, a): \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T est la transposée de X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Début du code ### (≈ 2 lignes de code)\n",
    "x_train_flatten = None\n",
    "x_test_flatten = None\n",
    "### Fin du code ###\n",
    "\n",
    "print (\"x_train_flatten shape: \" + str(x_train_flatten.shape))\n",
    "print (\"y_train shape: \" + str(y_train.shape))\n",
    "print (\"x_test_flatten shape: \" + str(x_test_flatten.shape))\n",
    "print (\"y_test shape: \" + str(y_test.shape))\n",
    "print (\"check 1 random après reshaping: \" + str(x_train_flatten[5:10,1]))\n",
    "print (\"check 2 random après reshaping: \" + str(x_train_flatten[17:22,34]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "    x_train_flatten shape: (12288, 209)\n",
    "    y_train shape: (1, 209)\n",
    "    x_test_flatten shape: (12288, 50)\n",
    "    y_test shape: (1, 50)\n",
    "    check 1 random après reshaping: [182 188 179 174 213]\n",
    "    check 2 random après reshaping: [20 16  3 22 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour représenter des images en couleurs, les channels rouges, verts et bleus (RGB) doivent être spécifiés pour chaque pixel, et la valeur de chaque pixel correspond en fait un vecteur de 3 nombres compris entre 0 et 255.\n",
    "\n",
    "Une étape de préprocessing assez courante en machine learning est de centrer et normaliser votre jeu de données, ce qui signifie que vous allez calculer la moyenne de tout le numpy array, puis diviser chaque exemple de ce dataset par l'écart-type. Mais pour les images, c'est plus simple et plus pratique et cela fonctionne tout aussi bien que de diviser uniquement chaque ligne du dataset par 255 (la valeur maximum d'un channel d'un pixel). Vous vous retrouvez alors avec un numpy array comprenant des nombres compris entre 0 et 1.\n",
    "\n",
    "Normalisons notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_flatten/255.\n",
    "x_test = x_test_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "\n",
    "Les étapes courants de préprocessing:\n",
    "- Analyser les données en affichant les dimensions et shapes du jeu de données (m_train, m_test, num_px, etc.)\n",
    "- Reshaper les datasets de façon à ce que chaque exemple devienne un vecteur de dimension (num_px \\* num_px \\* 3, 1)\n",
    "- Normaliser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Architecture générale de l'algorithme d'apprentissage ##\n",
    "\n",
    "C'est le moment de construire un simple algorithme pour reconnaître un chat d'un non-chat d'une image.\n",
    "\n",
    "Vous allez construire une régression logistique, tout en suivant le mindset (l'état d'esprit) d'un réseau de neurones. La figure suivante vous explique pourquoi **la régression logistique** est un réseau de neurones très simple !\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Expression mathématique de l'algorithme**:\n",
    "\n",
    "Pour chaque exemple $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "Le coût est ensuite calculé en additionnant toutes les loss de chaque exemple.\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Etapes clés**:\n",
    "Dans cet exercice, vous allez traiter les étapes suivantes:\n",
    "    - Initialiser les paramètres du modèle\n",
    "    - Apprendre les paramètres au modèle en minimisant le coût\n",
    "    - Utiliser les paramètres appris pour faire des prédictions (sur le jeu de données de test)\n",
    "    - Analyser les résultats et conclure\n",
    "\n",
    "N'hésitez pas à appeler l'un des assistants si vous avez besoin de plus de compréhension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Construire les différentes parties de l'algorithme ## \n",
    "\n",
    "Les étapes principales pour construire un réseau de neurones sont:\n",
    "1. Définir la structure du modèle (comme le nombre de features en entrée)\n",
    "2. Initialisez les paramètres du modèle\n",
    "3. Boucle:\n",
    "    - Calculer la loss actuelle (forward propagation)\n",
    "    - Calculer le gradient courant (backward propagation)\n",
    "    - Updater les paramètres (descente de gradient)\n",
    "    \n",
    "### 4.1 - Fonctions utiles\n",
    "\n",
    "**Exercise**: \n",
    "En utilisant votre code du workshop dernier (sur Python et numpy), implémenter la fonction sigmoïde. Comme vous l'avez vu dans la figure précédente, vous avez besoin de calculer $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ pour faire des prédictions. Utilisez np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- Un nombre scalaire ou un numpy array\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    s = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([-1, 0, 0.5, 2, 3]) = \" + str(sigmoid(np.array([-1, 0, 0.5, 2, 3]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "sigmoid([-1, 0, 0.5, 2, 3]) = [0.26894142 0.5        0.62245933 0.88079708 0.95257413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initialisation des paramètres\n",
    "\n",
    "**Exercise:** \n",
    "Implémentez l'initialisation des paramètres dans la cellule suivante. Vous devez initialiser w comme un vecteur ne contenant que des 0. Si vous ne savez pas quelle fonction numpy utiliser, renseignez vous sur np.zeros() dans la doc de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    \"\"\"\n",
    "    Cette fonction crée un vecteur de zeros de shape (dim, 1) pour w et initialise b à 0\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size du vecteur w qu'on veut (ou le nombre de paramètres dans ce cas)\n",
    "    \n",
    "    Returns:\n",
    "    w -- vecteur initialisé de shape (dim, 1)\n",
    "    b -- nombre scalaire initialisé correspondant au biais\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    w = None\n",
    "    b = None\n",
    "    ### Fin du code ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "w, b = initialize(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "w = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    " \n",
    "b = 0\n",
    "\n",
    "Pour les images, w doit être de shape (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward et Backward propagation\n",
    "\n",
    "Maintenant que vos paramètres sont initialisés, vous pouvez écrire les étapes de forward et backward propagation pour apprendre les paramètres.\n",
    "\n",
    "\n",
    "**Exercise:** Implémentez la fonction `propagate()` qui va calculer la fonction de coût et son gradient.\n",
    "\n",
    "**Indices**:\n",
    "\n",
    "Forward Propagation:\n",
    "- On vous donne X\n",
    "- Vous calculez $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Vous calculez la fonction de coût: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Voici les deux formules que vous utiliserez: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- les poids, un numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un nombre scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- le vecteur correspondant aux labels (0 si non-chat, 1 si chat) de size (1, nombre d'exemples)\n",
    "    \n",
    "    Return:\n",
    "    cost -- coût\n",
    "    dw -- gradient de la loss de même shape que w\n",
    "    db -- gradient de la loss de même shape que b\n",
    "    \n",
    "    Conseils:\n",
    "    - Ecrivez votre code étape par étape pour la propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward Propagation (de X à cost)\n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    A = None                                    # calcule l'activation\n",
    "    cost = None                                # compute cost\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    # Backward Propagation (pour trouver les gradients)\n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    dw = None\n",
    "    db = None\n",
    "    ### Fin du code ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[4.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagation(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  dw   </td>\n",
    "      <td> [[0.999923  ]\n",
    "         [2.39975403]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  db  </td>\n",
    "        <td> 7.288578855054369e-05 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  cost  </td>\n",
    "        <td> 8.800077000774023 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Optimisation\n",
    "\n",
    "- Vous avez initialisé vos paramètres.\n",
    "- Vous savez aussi calculer une fonction de coût et son gradient.\n",
    "- Maintenant, vous voulez update les paramètres en utilisant la descente de gradient.\n",
    "\n",
    "\n",
    "**Exercise:** Ecrivez la fonction d'optimisation. Le but est d'apprendre $w$ et $b$ en minimisant la fonction de coût $J$. Pour un paramètre $\\theta$ donné, la formule d'update est $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, où $\\alpha$ correspond au learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(w, b, X, Y, n_iterations, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- les poids, numpy array de size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    Y -- le vecteur correspondant aux labels (0 si non-chat, 1 si chat) de size (1, nombre d'exemples)\n",
    "    n_iterations -- nombre d'itérations dans la boucle d'optimisation\n",
    "    learning_rate -- le pas d'apprentissage\n",
    "    print_cost -- True pour afficher la loss toutes les 100 fois\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionnaire python contenant les poids w et le biais b\n",
    "    grads -- dictionnaire contenant les gradients de w et de b grâce à la fonction de coût\n",
    "    costs -- list de tous les coûts calculés pendant l'optimisation, cela nous permettra d'afficher la courbe d'apprentissage\n",
    "\n",
    "    Conseils:\n",
    "    Vous aurez besoin d'écrire 2 étapes et d'itérer à travers elles:\n",
    "        1) Calculez le coût et le gradient des paramètres. Utilisez propagation()\n",
    "        2) Updatez les paramètres en utilisant la descente de gradient pour w et b \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        \n",
    "        # Calcul du coût et du gradient (≈ 1-4 lignes de code)\n",
    "        ### Début du code ### \n",
    "        grads, cost = None\n",
    "        ### Fin du code ###\n",
    "        \n",
    "        # Récupérer les dérivées depuis grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update (≈ 2 lignes de code)\n",
    "        ### Début du code ###\n",
    "        w = None\n",
    "        b = None\n",
    "        ### Fin du code ###\n",
    "        \n",
    "        # Stockez les coûts\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Afficher le coût toutes les 100 itérations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coût après iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, costs = optimization(w, b, X, Y, n_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "w = [[ 3.11567021]\n",
    " [-0.10995646]]\n",
    "\n",
    "b = 1.9850788219795317\n",
    "\n",
    "dw = [[0.89699851]\n",
    " [2.07122651]]\n",
    "\n",
    "db = 0.09736680112994214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** La fonction précedente a permis d'afficher les poids w et le biais b appris. On peut les utiliser pour prédire les labels d'une dataset X. Implémenter la fonction `prediction()`. Il y a 2 étapes pour calculer les prédictions:\n",
    "\n",
    "1. Calculez $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convertissez les entrées de A en 0 (si activation <= 0) ou 1 (si activation > 0) et stockez les dans un vecteur `Y_prediction`. Si vous voulez, vous pouvez utiliser un `if`/`else` dans une boucle `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w, b, X):\n",
    "    '''\n",
    "    Arguments:\n",
    "    w -- les poids, numpy array de size (num_px * num_px * 3, 1)\n",
    "    b -- le biais, un scalaire\n",
    "    X -- matrice de size (num_px * num_px * 3, nombre d'exemples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- un numpy array (vecteur) contenant toutes les prédictions (0/1) des exemples de X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Calculez le vecteur \"A\" correspondant aux prédictions si un chat est présent dans l'image\n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    A = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convertissez les prédictions A[0,i] en p[0,i]\n",
    "        ### Début du code ### (≈ 4 lignes de code)\n",
    "        \n",
    "        ### Fin du code ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.2,-2.4],[1.4,2.5,0.6]])\n",
    "print (\"predictions = \" + str(prediction(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             predictions\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.  0.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "\n",
    "Vous avez implémenté plusieurs fonctions qui:\n",
    "- initialise (w, b)\n",
    "- optimise la loss de façon itérative puor apprendre les paramètres (w, b):\n",
    "    - calcul du coût et de son gradient\n",
    "    - update des paramètres en utilisant la descente de gradient\n",
    "- utilise les (w, b) appris pour prédire les labels d'un jeu de données d'exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Mergez toutes les fonctions dans un modèle unique ##\n",
    "\n",
    "Vous allez maintenant voir comment le modèle global est structure en intégrant tous les blocs (les fonctions que vous avez implémenté) ensemble, dans le bon ordre.\n",
    "\n",
    "**Exercice**: Implémentez la fonction \"model\". Utilisez les notations suivantes:\n",
    "    - Y_pred_test pour vos prédictions sur le jeu de données de test\n",
    "    - Y_pred_train pour vos prédictions sur le jeu de données de tain\n",
    "    - w, costs, grads pour les outputs de optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, n_iterations = 2000, learning_rate = 0.5, print_cost=False):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    X_train -- jeu de données de train représenté par un numpy array de shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- labels des données de train représenté par un numpy array (vecteur) of shape (1, m_train)\n",
    "    X_test -- jeu de données de test représenté par un numpy array de shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- labels des données de test représenté par un numpy array (vecteur) of shape (1, m_test)\n",
    "    n_iterations -- nombre d'itérations dans la boucle d'optimisation\n",
    "    learning_rate -- le pas d'apprentissage\n",
    "    print_cost -- True pour afficher la loss toutes les 100 fois\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    d -- dictionnaire python contenant toutes les informations du modèle.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ###\n",
    "    \n",
    "    # Initialisez les paramètres avec des 0 (≈ 1 ligne de code)\n",
    "    w, b = None\n",
    "\n",
    "    # Descente de gradient (≈ 1 ligne de code)\n",
    "    parameters, grads, costs = None\n",
    "    \n",
    "    # récupérez les paramètres depuis parameters\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Prédisez les exemples de train et de test (≈ 2 lignes de code)\n",
    "    Y_pred_test = None\n",
    "    Y_pred_train =  None\n",
    "\n",
    "    ### Fin du code ###\n",
    "\n",
    "    # Print les erreurs de train et de test\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_pred_test\": Y_pred_test, \n",
    "         \"Y_pred_train\" : Y_pred_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": n_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la cellule suivante pour entraîner votre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(x_train, y_train, x_test, y_test, n_iterations = 2000, learning_rate = 0.006, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "Coût après iteration 0: 0.693147\n",
    "\n",
    "Coût après iteration 100: 0.649811\n",
    "\n",
    "Coût après iteration 200: 0.538312\n",
    "\n",
    "Coût après iteration 300: 0.439262\n",
    "\n",
    "Coût après iteration 400: 0.349825\n",
    "\n",
    "Coût après iteration 500: 0.278498\n",
    "\n",
    "Coût après iteration 600: 0.249764\n",
    "\n",
    "Coût après iteration 700: 0.231178\n",
    "\n",
    "Coût après iteration 800: 0.215229\n",
    "\n",
    "Coût après iteration 900: 0.201339\n",
    "\n",
    "Coût après iteration 1000: 0.189110\n",
    "\n",
    "Coût après iteration 1100: 0.178249\n",
    "\n",
    "Coût après iteration 1200: 0.168533\n",
    "\n",
    "Coût après iteration 1300: 0.159788\n",
    "\n",
    "Coût après iteration 1400: 0.151873\n",
    "\n",
    "Coût après iteration 1500: 0.144677\n",
    "\n",
    "Coût après iteration 1600: 0.138104\n",
    "\n",
    "Coût après iteration 1700: 0.132079\n",
    "\n",
    "Coût après iteration 1800: 0.126537\n",
    "\n",
    "Coût après iteration 1900: 0.121421\n",
    "\n",
    "train accuracy: 99.52153110047847 %\n",
    "\n",
    "test accuracy: 68.0 %\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Commentaire**: Votre accurary de train est très proche de 100%. C'est très bien, votre modèle est fonctionnel et sait très bien reconnaître les chats du jeu de données de train. Cependant, l'accuracy de test est de 68%. C'est plutôt pas si mal pour le modèle simple qu'on a construit étant donné le petit jeu de données fourni, mais ne vous inquietez pas, on construira un meilleur modèle dans un futur workshop.\n",
    "\n",
    "Aussi, vous avez remarqué que le modèle a clairement overfitté les données de train. On verra plus tard des méthodes pour réduire l'overfitting, en utilisant la régularisation par exemple). Utilisez le code ci-dessous pour afficher la fonction de coût et ses gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (par centaine)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Interpretation**:\n",
    "Vous pouvez voir que le coût décroît. Cela montre que les paramètres sont en cours d'apprentissage. Cependant, vous voyez que le modèle s'entraîne trop sur le jeu de données de train. Essayer d'augmenter le nombre d'itérations dans la cellule au dessus et re-exécutez les cellules. Vous devriez voir que l'accurary du training set augmente mais que celui de test baisse. C'est ce qu'on appelle l'overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques sources utiles:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
