{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Numpy basiques\n",
    "\n",
    "Bienvenue à votre premier workshop IA organisé par PoC !\n",
    "Ce notebook vous donne une brève introduction au langage Python. Même si vous avez déjà codé en Python avant, cela vous sera utile pour vous familiariser avec les fonctions dont vous aurez besoin.  \n",
    "\n",
    "**Instructions:**\n",
    "- Vous utiliserez Python 3\n",
    "- N'utilisez pas de boucle 'for' ou 'while', à moins qu'on ne vous le demande de façon explicite.\n",
    "- Après avoir codé votre fonction, lancez la cellule qui suit afin de vérifier votre résultat.\n",
    "\n",
    "**A la fin de ce workshop, vous saurez:**\n",
    "- Utiliser des notebooks iPython\n",
    "- Utiliser les fonctions de numpy et les opérations vectorielles et matricielles\n",
    "- Comprendre le concept de broadcasting\n",
    "- Vectoriser du code\n",
    "\n",
    "Si vous vous sentez bloqués ou ne comprenez pas un concept, n'hésitez pas à venir nous voir. **Nous sommes là pour ça !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A propos des Notebooks IPython ##\n",
    "\n",
    "Les notebooks iPython sont des environnemments de code interactif au sein d'une page web. Ils vous permettent d'éxécuter du code pas à pas et de pouvoir visualiser directement le contenu d'une variable ou un résultat par exemple. Vous utiliserez des notebooks iPython lors de vos workshops. Vous avez uniquement besoin d'écrire du code entre les commentaires ### Début du code ### et ### Fin du code ###. Après avoir écrit votre code, vous pouvez exécuter la cellule soit en entrant SHIFT+ENTREE, soit en cliquant sur 'Exécuter' (symbole play dans la barre de haut).\n",
    "\n",
    "Vous rencontrerez souvent des \"≈ X lignes de code\". Il ne s'agit que d'une estimation du nombre de lignes nécessaire, libre à vous d'en écrire plus ou moins.\n",
    "\n",
    "**Exercise**: Définissez la variable 'test' en 'Hello world !' pour afficher 'Hello world !' et éxécutez les deux cellules qui suivent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Début du code ### (≈ 1 ligne de code)\n",
    "test = None\n",
    "### Fin du code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "test: Hello world !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir**:\n",
    "- Exécutez vos cellules en entrant SHIFT+ENTREE (ou en cliquant sur Exécuter)\n",
    "- Ecrire votre code uniquement dans les cellules prévues à cet effet\n",
    "- Ne pas modifier les autres cellules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Créer des fonctions basiques avec Numpy ##\n",
    "\n",
    "Numpy est le package officiel pour le calcul scientifique en Python. Il est maintenu par une large communauté (www.numpy.org).\n",
    "Dans cet exercice, vous allez apprendre plusieurs fonctions clés de Numpy tels que np.exp, np.log et np.reshape. Retenez les bien car vous en aurez besoin pour les futurs workshops.\n",
    "\n",
    "### 1.1 - La fonction sigmoïde, np.exp() ###\n",
    "\n",
    "Avant d'utiliser np.exp(), vous utiliserez math.exp() pour implémenter la fonction sigmoïde. Vous comprendrez ainsi pourquoi np.exp() est préférable à math.exp().\n",
    "\n",
    "\n",
    "**Exercise**: Créez une fonction qui retourne la sigmoïde d'une nombre réel x. Utilisez math.exp() pour la fonction exponentielle.\n",
    "\n",
    "**Rappel**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ est aussi connue parfois sous le nom de fonction logistique. Il s'agit d'une fonction non linéaire qui n'est pas utilisée uniquement en Machine Learning (cf. régression logistique), mais aussi en Deep Learning.\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "Pour se référer à une fonction appartenant à un package spécifique, vous pouvez l'appeler en utilisant nom_du_package.fonction(). Exécutez le code ci-dessous pour voir un exemple avec math.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid_math(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- un nombre scalaire\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid_math(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    s = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_math(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "0.9241418199787566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait, on utilise rarement la librairie \"math\" en deep learning parce que les inputs des fonctions sont des nombres réels. En deep learning, on va plutôt utiliser des matrices et des vecteurs. C'est là qu'on aperçoit l'utilité de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Une des raisons pourquoi on utilise \"numpy\" au lieu de \"math\" ###\n",
    "x = [1, 2, 3]\n",
    "sigmoid_math(x) # vous verrez une erreur s'afficher si vous éxécutez la cellule. \n",
    "                # Ceci est dû au fait que x est un vecteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait, si $ x = (x_1, x_2, ..., x_n)$ est un vecteur, alors $np.exp(x)$ va appliquer la fonction exponentielle à chaque élément de x. Le résultat sera alors: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example de np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # le résultat est (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, si x est un vecteur, alors une opération Python comme $s = x + 3$ ou $s = \\frac{1}{x}$ donnera en sortie s un vecteur de la même dimension que x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple d'une opération vectorielle\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque fois que vous aurez besoin d'une information supplémentaire sur une fonction numpy, nous vous encourageons d'aller jeter un oeil à [la doc officielle](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "Vous pouvez aussi créer une nouvelle cellule dans ce notebook et écrire `np.exp?` (par exemple) pour avoir un accès rapide à la doc.\n",
    "\n",
    "**Exercise**: Implémentez la fonction sigmoïde en utilisant numpy.\n",
    "\n",
    "**Instructions**: x peut être un nombre réel, un vecteur, ou une matrice. La structure de données qu'on utilise dans numpy pour représenter ces dimensions (vecteurs, matrices, etc.) sont appelés des numpy array. Vous n'avez pas besoin d'en savoir plus pour l'instant.\n",
    "$$ \\text{Pour tout x } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # np est un raccourci pour numpy. Cela vous permettra de taper uniquement np.exp() au lieu de numpy.exp()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- un nombre scalaire ou un numpy array de n'importe quelle taille\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    s = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.5, 2.5, 3.5])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "array([0.81757448, 0.92414182, 0.97068777])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Gradient de la sigmoïde\n",
    "\n",
    "Comme vous l'avez entendu au début du workshop, vous aurez besoin de calculer les gradients afin d'optimiser la fonction de coût en utilisant la backpropagation.\n",
    "\n",
    "**Exercise**: Implémentez la fonction sigmoid_gradient pour calculer le gradient de la fonction sigmoïde. La formule est la suivante: $$dérivée\\_sigmoïde(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "Vous allez coder cette fonction en 2 étapes:\n",
    "1. Définissez s comme la sigmoïde de x. La fonction sigmoid(x) peut vous être utile.\n",
    "2. Calculez $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    \"\"\"    \n",
    "    Arguments:\n",
    "    x -- un nombre scalaire ou un numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- votre gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    s = None\n",
    "    ds = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.5, 2.5, 3.5])\n",
    "print (\"sigmoid_deriv(x) = \" + str(sigmoid_deriv(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "sigmoid_deriv(x) = [0.14914645 0.07010372 0.02845302]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Redimensionner les arrays ###\n",
    "\n",
    "Deux fonctions numpy courantes utilisées en deep learning sont [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) et [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape vous permet d'avoir le shape (dimension) d'une matrice ou d'un vecteur X.\n",
    "- X.reshape(...) est utilisé pour redimensionner X en une autre dimension.\n",
    "\n",
    "Par exemple, en informatique, une image est représentée par un array de 3 dimensions $(longueur, largeur, profondeur = 3)$.\n",
    "Cependant, quand vous donnez une image en entrée d'un algorithme, vous devez le convertir en un vecteur de shape (longueur\\*largeur\\*3, 1). En d'autres mots, vous enroulez, ou redimensionnez, l'array 3D en un vecteur 1D.\n",
    "\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implementez `image_to_vector()` qui prend en entrée une matrice de shape (longueur, largeur, 3) et return un vecteur de shape (longueur\\*largeur\\*3, 1). Par exemple, si vous voulez reshape un array v de shape (a, b, c) en un vecteur de shape (a\\*b, c), vous ferez:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- S'il vous plaît, ne harcodez pas les dimensions de l'image comme une constante ! Regardez plutôt les valeurs dont vous avez besoin avec `image.shape[0]`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- un numpy array de shape (longueur, largeur, profondeur)\n",
    "    \n",
    "    Returns:\n",
    "    v -- un vecteur de shape (longueur*largeur*profondeur, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    v = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il s'agit d'un array de shape (3, 3, 2). En général, les images seront de shape (nb_pix_x, nb_pix_y, 3) où 3 représenteront les valeurs RGB\n",
    "image = np.array([[[ 0.67126139,  0.29381281],\n",
    "        [ 0.90714982,  0.52835547],\n",
    "        [ 0.42245251 ,  0.45012151]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85114703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.6213595,  0.00531265],\n",
    "        [ 0.1210313,  0.49974237],\n",
    "        [ 0.3432129,  0.94631277]]])\n",
    "print (\"image_to_vector(image) = \" + str(image_to_vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "image_to_vector(image) = [[0.67126139]\n",
    " [0.29381281]\n",
    " [0.90714982]\n",
    " [0.52835547]\n",
    " [0.42245251]\n",
    " [0.45012151]\n",
    " [0.92814219]\n",
    " [0.96677647]\n",
    " [0.85114703]\n",
    " [0.52351845]\n",
    " [0.19981397]\n",
    " [0.27417313]\n",
    " [0.6213595 ]\n",
    " [0.00531265]\n",
    " [0.1210313 ]\n",
    " [0.49974237]\n",
    " [0.3432129 ]\n",
    " [0.94631277]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalisez vos données\n",
    "\n",
    "Une autre technique courante qu'on utilise en Machine Learning et Deep Learning est de normaliser nos données. Cela permet souvent d'apporter une meilleure perfomance parce que la descente de gradient convergera plus rapidement après la normalisation. Ici, par normalisation on signifie changer x par $ \\frac{x}{\\| x\\|} $ (diviser chaque vecteur de x par sa norme).\n",
    "\n",
    "Par exemple, si $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ alors $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$et        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "Notez que vous pouvez diviser des matrices de tailles différentes, et cela fonctionne bien: il s'agit du broadcasting dont nous en parlerons dans la section 1.5.\n",
    "\n",
    "\n",
    "**Exercise**: Implementez normalize_rows() pour normaliser les lignes d'une matrice. Après avoir appliqué cette function à une matrice x, chaque ligne de x doit être un vecteur de taille unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- une matrice numpy x de shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- la matrice normalisée x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 2 lignes de code)\n",
    "    # Calculez d'abord la norme de x. Utilisez np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = None\n",
    "    \n",
    "    # Divisez x par x_norm.\n",
    "    x = None\n",
    "    ### Fin du code ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [2, 3, 6],\n",
    "    [5, 2, 8]])\n",
    "print(\"normalize_rows(x) = \" + str(normalize_rows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "\n",
    "normalize_rows(x) = [[0.28571429 0.42857143 0.85714286]\n",
    " [0.51847585 0.20739034 0.82956136]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Dans la fonction normalize_rows, vous pouvez essayer de print les shapes de x_norm et de x, et répéter l'action autant de fois que vous voulez. Vous remarquerez qu'ils peuvent avoir des shapes différents. C'est normal car x_norm prend la norme de chaque ligne de x. Donc x_norm a le même nombre de lignes, mais qu'une seule colonne. Comment cela fonctionne quand vous divisez x par x_norm ? Il s'agit du broadcasting et nous en parlons tout de suite ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Le broadcasting et la fonction softmax ####\n",
    "\n",
    "Le broadcasting est un concept très important à comprendre dans numpy. Il s'agit d'une notion très utile pour pouvoir faire des opérations mathématiques entre des arrays de shapes différents. Pour plus d'informations, vous pouvez consulter [la documentation du broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implémentez la fonction softmax en utilisant numpy. Vous pouvez voir softmax comme une fonction de normalisation utilisée quand votre algorithme a besoin de classifier 2 classes ou +. Vous en apprendrez plus sur cette fonction dans un futur workshop.\n",
    "\n",
    "Attention: Votre code doit pouvoir fonctionner avec un vecteur, mais aussi une matrice\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{Pour tout } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{Pour une matrice } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ correspond à l'élément de la $i^{ième}$ ligne et de la  $j^{ième}$ colonne de $x$, on a donc: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(première ligne de x)}  \\\\\n",
    "    softmax\\text{(deuxième ligne de x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(dernière ligne de x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    x -- Un vecteur ou une matrice numpy de shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- Une matrice numpy égale au softmax de x, de shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 3 lignes de code)\n",
    "    # Calculez l'exponentielle de chaque élément de x. Utilisez np.exp()\n",
    "    x_exp = None\n",
    "\n",
    "    # Créez un vecteur x_sum qui fait la somme de chaque ligne de x_exp. Utilisez np.sum(..., axis = 1, keepdims = True)\n",
    "    x_sum = None\n",
    "    \n",
    "    # Calculer softmax(x) en divisant x_exp par x_sum. Cela utilisera automatiquement le broadcasting de numpy.\n",
    "    s = x_exp / x_sum\n",
    "    ### Fin du code ###\n",
    "    print(x_exp.shape, x_sum.shape, s.shape)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vect = np.array([[9, 4, 0, 0 ,0]]) #sisi 94 RPZ\n",
    "\n",
    "print(\"softmax(x_vect) = \" + str(softmax(x_vect)))\n",
    "\n",
    "x_matr = np.array([\n",
    "    [1, 7, 5, 0, 6],\n",
    "    [3, 4, 0, 2 ,0]])\n",
    "\n",
    "print(\"softmax(x_matr) = \" + str(softmax(x_matr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "\n",
    "softmax(x_vect) = [[9.92941993e-01 6.69039052e-03 1.22538777e-04 1.22538777e-04\n",
    "  1.22538777e-04]]\n",
    "\n",
    "softmax(x_matr) = [[1.64525645e-03 6.63743823e-01 8.98279582e-02 6.05256022e-04\n",
    "  2.44177707e-01]\n",
    " [2.38906644e-01 6.49415590e-01 1.18944614e-02 8.78888428e-02\n",
    "  1.18944614e-02]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- Si vous printez les shapes de x_exp, x_sum et de s ci-dessus et reéxécutez la cellule, dans le cas de la matrice vous verrez que x_sum est de shape (2, 1) alors que x_exp et s sont de shape (2, 5). **x_exp/x_sum** fonctionne correctement grâce au broadcasting.\n",
    "\n",
    "Bravo ! Vous avez maintenant acquis une bonne compréhension de python et de sa librairie numpy, et avez implémenté quelques fonctions qui vous seront utiles en deep learning !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "- np.exp(x) fonctionne pour n'importe quel np.array x et applique la fonction exponentielle à tous ses éléments\n",
    "- la fonction sigmoïde et son gradient\n",
    "- image_to_vector est couramment utilisé en deep learning\n",
    "- np.reshape est souvent utilisé. Plus tard, vous verrez que sa bonne utilisation vous permettra d'éviter de nombreux bugs\n",
    "- numpy a des functions de built-in très efficaces\n",
    "- le broadcasting est extrêmement utile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) La vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning, on manipule de très gros datasets. De ce fait, une fonction de calcul non optimisée peut présenter un poids lourd à votre algorithme et produira un modèle qui mettrait des années à s'éxécuter. Pour être sûr que votre code est suffisamment optimisé en terme de calcul, vous utiliserez le concept de vectorisation. Par exemple, essayez de me dire quelle est la différence entre les différentes implémentations de produits dot/outer/elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### Implémentation classique d'un produit dot de vecteurs ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation classique d'un produit outer ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # on crée une matrice de taille len(x1)*len(x2) matrix avec que des 0\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation classique élément-wise ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"multiplication élément-wise = \" + str(mul) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation classique général d'un produit dot ###\n",
    "W = np.random.rand(3, len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### Implémentation numpy du produit dot ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation numpy de l'outer ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation numpy de l'élement-wise ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\\n\")\n",
    "\n",
    "### Implémentation numpy du dot général ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Temps de calcul = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous avez pu le remarquer, l'implémentation numpy est plus propre et plus efficace. Pour des vecteurs et matrices de taille supérieure, les différences en temps de calcul deviennent plus grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implémentez les fonctions de loss L1 et L2\n",
    "\n",
    "**Exercise**: Implémentez la version numpy de la fonction de loss L1. La fonction abs(x) (valeur absolue de x) peut s'avérer très utile.\n",
    "\n",
    "\n",
    "**Rappel**:\n",
    "- La loss est utilisée pour évaluer la performance de votre modèle. Plus la loss est grande, plus vos prédictions ($ \\hat{y} $) sont éloignées des vraies valeurs ($y$). En deep learning, vous utilisez des algorithmes d'optimisation comme la descente de gradient afin d'entraîner votre modèle et de minimiser le coût.\n",
    "- L1 est défini comme suit:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_function(y_predicted, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_predicted -- vecteur de size m contenant vos valeurs prédites\n",
    "    y -- vecteur de size m contenant les vraies valeurs\n",
    "    \n",
    "    Returns:\n",
    "    L1 -- valeur de la fonction de loss L1\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    L1 = None\n",
    "    ### Fin du code ###\n",
    "    \n",
    "    return L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.array([.8, 0.3, 0.2, .6, .2])\n",
    "y = np.array([1, 1, 0, 1, 0])\n",
    "print(\"L1 = \" + str(L1_function(y_predicted,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "L1 = 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implémentez la version vectorisée de la fonction de loss L2. Il y a plusieurs manières de le faire. Vous pourrez trouver la fonction np.dot() utile. Pour rappel, si $x = [x_1, x_2, ..., x_n]$, alors `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "\n",
    "- La fonction de loss L2 est définie comme suit: $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_function(y_predicted, y):\n",
    "    ### Début du code ### (≈ 1 ligne de code)\n",
    "    L2 = None\n",
    "    ### Fin du coee ###\n",
    "    \n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.array([.8, 0.3, 0.2, .6, .2])\n",
    "y = np.array([1, 1, 0, 1, 0])\n",
    "print(\"L2 = \" + str(L2_function(y_predicted,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**: \n",
    "L2 = 0.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Félicitations, vous avez terminé ce workshop ! On espère que ce petit exercice vous a permis de bien vous familiariser avec Python et numpy, ce qui constitue de très bonnes bases pour les prochains workshops (qui seront bien plus intéressants). A la semaine prochaine ! ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Ce que vous devez retenir:**\n",
    "- La vectorisation est très importante en deep learning. Elle permet une meilleure performance de calcul et un code plus lisible\n",
    "- Les fonctions L1 et L2\n",
    "- Vous pouvez maintenant voir d'autres fonctions numpy comme np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
